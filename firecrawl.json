{
  "name": "Firecrawl",
  "description": "# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/mendableai/firecrawl) for web scraping capabilities.\n\n## Features\n\n- Web scraping, crawling, and discovery\n- Search and content extraction\n- Deep research and batch scraping\n- Automatic retries and rate limiting\n- SSE support\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## How to Choose a Tool\n\nUse this guide to select the right tool for your task:\n\n- **If you know the exact URL(s) you want:**\n  - For one: use **scrape**\n  - For many: use **batch_scrape**\n- **If you need to discover URLs on a site:** use **map**\n- **If you want to search the web for info:** use **search**\n- **If you want to extract structured data:** use **extract**\n- **If you want to analyze a whole site or section:** use **crawl** (with limits!)\n- **If you want to do in-depth research:** use **deep_research**\n- **If you want to generate LLMs.txt:** use **generate_llmstxt**\n\n### Quick Reference Table\n\n| Tool                | Best for                                 | Returns         |\n|---------------------|------------------------------------------|-----------------|\n| scrape              | Single page content                      | markdown/html   |\n| batch_scrape        | Multiple known URLs                      | markdown/html[] |\n| map                 | Discovering URLs on a site               | URL[]           |\n| crawl               | Multi-page extraction (with limits)      | markdown/html[] |\n| search              | Web search for info                      | results[]       |\n| extract             | Structured data from pages               | JSON            |\n| deep_research       | In-depth, multi-source research          | summary, sources|\n| generate_llmstxt    | LLMs.txt for a domain                    | text            |\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n**Best for:**\n- Single page content extraction, when you know exactly which page contains the information.\n\n**Not recommended for:**\n- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)\n- When you're unsure which page contains the information (use search)\n- When you need structured data (use extract)\n\n**Common mistakes:**\n- Using scrape for a list of URLs (use batch_scrape instead).\n\n**Prompt Example:**\n> \"Get the content of the page at https://example.com.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n**Returns:**\n- Markdown, HTML, or other formats as specified.\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n**Best for:**\n- Retrieving content from multiple pages, when you know exactly which pages to scrape.\n\n**Not recommended for:**\n- Discovering URLs (use map first if you don't know the URLs)\n- Scraping a single page (use scrape)\n\n**Common mistakes:**\n- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)\n\n**Prompt Example:**\n> \"Get the content of these three blog posts: [url1, url2, url3].\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Map Tool (`firecrawl_map`)\n\nMap a website to discover all indexed URLs on the site.\n\n**Best for:**\n- Discovering URLs on a website before deciding what to scrape\n- Finding specific sections of a website\n\n**Not recommended for:**\n- When you already know which specific URL you need (use scrape or batch_scrape)\n- When you need the content of the pages (use scrape after mapping)\n\n**Common mistakes:**\n- Using crawl to discover URLs instead of map\n\n**Prompt Example:**\n> \"List all URLs on example.com.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_map\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Returns:**\n- Array of URLs found on the site\n\n### 5. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n**Best for:**\n- Finding specific information across multiple websites, when you don't know which website has the information.\n- When you need the most relevant content for a query\n\n**Not recommended for:**\n- When you already know which website to scrape (use scrape)\n- When you need comprehensive coverage of a single website (use map or crawl)\n\n**Common mistakes:**\n- Using crawl or map for open-ended questions (use search instead)\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"latest AI research papers 2023\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n- Array of search results (with optional scraped content)\n\n**Prompt Example:**\n> \"Find the latest research papers on AI published in 2023.\"\n\n### 6. Crawl Tool (`firecrawl_crawl`)\n\nStarts an asynchronous crawl job on a website and extract content from all pages.\n\n**Best for:**\n- Extracting content from multiple related pages, when you need comprehensive coverage.\n\n**Not recommended for:**\n- Extracting content from a single page (use scrape)\n- When token limits are a concern (use map + batch_scrape)\n- When you need fast results (crawling can be slow)\n\n**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.\n\n**Common mistakes:**\n- Setting limit or maxDepth too high (causes token overflow)\n- Using crawl for a single page (use scrape instead)\n\n**Prompt Example:**\n> \"Get all blog posts from the first two levels of example.com/blog.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com/blog/*\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n**Returns:**\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Check Crawl Status (`firecrawl_check_crawl_status`)\n\nCheck the status of a crawl job.\n\n```json\n{\n  \"name\": \"firecrawl_check_crawl_status\",\n  \"arguments\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }\n}\n```\n\n**Returns:**\n- Response includes the status of the crawl job:\n  \n### 8. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n**Best for:**\n- Extracting specific structured data like prices, names, details.\n\n**Not recommended for:**\n- When you need the full content of a page (use scrape)\n- When you're not looking for specific structured data\n\n**Arguments:**\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nFor extraction, the MCP server will use it uses Firecrawl's managed LLM service.\n**Prompt Example:**\n> \"Extract the product name, price, and description from these product pages.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\n**Returns:**\n- Extracted structured data as defined by your schema\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 9. Deep Research Tool (`firecrawl_deep_research`)\n\nConduct deep web research on a query using intelligent crawling, search, and LLM analysis.\n\n**Best for:**\n- Complex research questions requiring multiple sources, in-depth analysis.\n\n**Not recommended for:**\n- Simple questions that can be answered with a single search\n- When you need very specific information from a known page (use scrape)\n- When you need results quickly (deep research can take time)\n\n**Arguments:**\n- query (string, required): The research question or topic to explore.\n- maxDepth (number, optional): Maximum recursive depth for crawling/search (default: 3).\n- timeLimit (number, optional): Time limit in seconds for the research session (default: 120).\n- maxUrls (number, optional): Maximum number of URLs to analyze (default: 50).\n\n**Prompt Example:**\n> \"Research the environmental impact of electric vehicles versus gasoline vehicles.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_deep_research\",\n  \"arguments\": {\n    \"query\": \"What are the environmental impacts of electric vehicles compared to gasoline vehicles?\",\n    \"maxDepth\": 3,\n    \"timeLimit\": 120,\n    \"maxUrls\": 50\n  }\n}\n```\n\n**Returns:**\n- Final analysis generated by an LLM based on research. (data.finalAnalysis)\n- May also include structured activities and sources used in the research process.\n\n### 10. Generate LLMs.txt Tool (`firecrawl_generate_llmstxt`)\n\nGenerate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact \nwith the site.\n\n**Best for:**\n- Creating machine-readable permission guidelines for AI models.\n\n**Not recommended for:**\n- General content extraction or research\n\n**Arguments:**\n- url (string, required): The base URL of the website to analyze.\n- maxUrls (number, optional): Max number of URLs to include (default: 10).\n- showFullText (boolean, optional): Whether to include llms-full.txt contents in the response.\n\n**Prompt Example:**\n> \"Generate an LLMs.txt file for example.com.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_generate_llmstxt\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"maxUrls\": 20,\n    \"showFullText\": true\n  }\n}\n```\n\n**Returns:**\n- LLMs.txt file contents (and optionally llms-full.txt)\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n### Thanks to contributors\n\nThanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n\nThanks to MCP.so and Klavis AI for hosting and [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) and [@zihaolin96](https://github.com/zihaolin96) for integrating our server.\n\n## License\n\nMIT License - see [LICENSE](https://raw.githubusercontent.com/mendableai/firecrawl-mcp-server/refs/heads/main/LICENSE) file for details\n",
  "metadata": {
    "categories": "Retrieval & Search,Automation & Browsers,Science & Research,Verified",
    "source": "vendor" 
  },
  "icon": "https://avatars.githubusercontent.com/u/135057108?v=4",
  "repoURL": "https://github.com/mendableai/firecrawl-mcp-server",
  "toolPreview": [
    {
      "name": "firecrawl_scrape",
      "description": "Scrape content from a single URL with advanced options.",
      "params": {
        "actions": "List of actions to perform before scraping",
        "excludeTags": "HTML tags to exclude from extraction",
        "extract": "Configuration for structured data extraction",
        "formats": "Content formats to extract (default: ['markdown'])",
        "includeTags": "HTML tags to specifically include in extraction",
        "location": "Location settings for scraping",
        "maxAge": "Maximum age in milliseconds for cached content. Use cached data if available and younger than maxAge, otherwise scrape fresh. Enables 500% faster scrapes for recently cached pages. Default: 0 (always scrape fresh)",
        "mobile": "Use mobile viewport",
        "onlyMainContent": "Extract only the main content, filtering out navigation, footers, etc.",
        "removeBase64Images": "Remove base64 encoded images from output",
        "skipTlsVerification": "Skip TLS certificate verification",
        "timeout": "Maximum time in milliseconds to wait for the page to load",
        "url": "The URL to scrape",
        "waitFor": "Time in milliseconds to wait for dynamic content to load"
      }
    },
    {
      "name": "firecrawl_map",
      "description": "Map a website to discover all indexed URLs on the site.",
      "params": {
        "ignoreSitemap": "Skip sitemap.xml discovery and only use HTML links",
        "includeSubdomains": "Include URLs from subdomains in results",
        "limit": "Maximum number of URLs to return",
        "search": "Optional search term to filter URLs",
        "sitemapOnly": "Only use sitemap.xml for discovery, ignore HTML links",
        "url": "Starting URL for URL discovery"
      }
    },
    {
      "name": "firecrawl_crawl",
      "description": "Starts an asynchronous crawl job on a website and extracts content from all pages.",
      "params": {
        "allowBackwardLinks": "Allow crawling links that point to parent directories",
        "allowExternalLinks": "Allow crawling links to external domains",
        "deduplicateSimilarURLs": "Remove similar URLs during crawl",
        "excludePaths": "URL paths to exclude from crawling",
        "ignoreQueryParameters": "Ignore query parameters when comparing URLs",
        "ignoreSitemap": "Skip sitemap.xml discovery",
        "includePaths": "Only crawl these URL paths",
        "limit": "Maximum number of pages to crawl",
        "maxDepth": "Maximum link depth to crawl",
        "scrapeOptions": "Options for scraping each page",
        "url": "Starting URL for the crawl",
        "webhook": ""
      }
    },
    {
      "name": "firecrawl_check_crawl_status",
      "description": "Check the status of a crawl job.",
      "params": {
        "id": "Crawl job ID to check"
      }
    },
    {
      "name": "firecrawl_search",
      "description": "Search the web and optionally extract content from search results.",
      "params": {
        "country": "Country code for search results (default: us)",
        "filter": "Search filter",
        "lang": "Language code for search results (default: en)",
        "limit": "Maximum number of results to return (default: 5)",
        "location": "Location settings for search",
        "query": "Search query string",
        "scrapeOptions": "Options for scraping search results",
        "tbs": "Time-based search filter"
      }
    },
    {
      "name": "firecrawl_extract",
      "description": "Extract structured information from web pages using LLM capabilities.",
      "params": {
        "allowExternalLinks": "Allow extraction from external links",
        "enableWebSearch": "Enable web search for additional context",
        "includeSubdomains": "Include subdomains in extraction",
        "prompt": "Prompt for the LLM extraction",
        "schema": "JSON schema for structured data extraction",
        "systemPrompt": "System prompt for LLM extraction",
        "urls": "List of URLs to extract information from"
      }
    },
    {
      "name": "firecrawl_generate_llmstxt",
      "description": "Generate a standardized llms.txt (and optionally llms-full.txt) file for a given domain. This file defines how large language models should interact with the site",
      "params": {
        "maxUrls": "Maximum number of URLs to process (1-100, default: 10)",
        "showFullText": "Whether to show the full LLMs-full.txt in the response",
        "url": "The URL to generate LLMs.txt from"
      }
    }
  ],
  "env": [
    {
      "key": "FIRECRAWL_API_KEY",
      "name": "Firecrawl API Key",
      "required": true,
      "sensitive": true,
      "description": "Your Firecrawl API key."
    }
  ],
  "args": [
    "-y",
    "firecrawl-mcp"
  ],
  "command": "npx"
}